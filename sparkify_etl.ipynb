{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import packages and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read configuration file and get AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Spark Cluster Setup (IN TERMINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Create an EMR Cluster with 3 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "client = boto3.client('emr', region_name='us-east-1')\n",
    "\n",
    "response = client.run_job_flow(\n",
    "    Name=\"Boto3 test cluster\",\n",
    "    ReleaseLabel='emr-6.3.0',\n",
    "    Instances={\n",
    "        'KeepJobFlowAliveWhenNoSteps': True,\n",
    "        'TerminationProtected': False,\n",
    "        'InstanceGroups': [\n",
    "            {\n",
    "                'Name': 'Master',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'MASTER',\n",
    "                'InstanceType': 'm4.large',\n",
    "                'InstanceCount': 1,\n",
    "                'EbsConfiguration': {\n",
    "                    'EbsBlockDeviceConfigs': [\n",
    "                        {\n",
    "                            'VolumeSpecification': {\n",
    "                                'VolumeType': 'gp2',\n",
    "                                'SizeInGB': 10\n",
    "                            },\n",
    "                            'VolumesPerInstance': 1\n",
    "                        },\n",
    "                    ],\n",
    "                    'EbsOptimized': False\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Name': 'Core',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'CORE',\n",
    "                'InstanceType': 'm4.large',\n",
    "                'InstanceCount': 1,\n",
    "                'EbsConfiguration': {\n",
    "                    'EbsBlockDeviceConfigs': [\n",
    "                        {\n",
    "                            'VolumeSpecification': {\n",
    "                                'VolumeType': 'gp2',\n",
    "                                'SizeInGB': 10\n",
    "                            },\n",
    "                            'VolumesPerInstance': 1\n",
    "                        },\n",
    "                    ],\n",
    "                    'EbsOptimized': False\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'Name': 'Task',\n",
    "                'Market': 'ON_DEMAND',\n",
    "                'InstanceRole': 'TASK',\n",
    "                'InstanceType': 'm4.large',\n",
    "                'InstanceCount': 1,\n",
    "                'EbsConfiguration': {\n",
    "                    'EbsBlockDeviceConfigs': [\n",
    "                        {\n",
    "                            'VolumeSpecification': {\n",
    "                                'VolumeType': 'gp2',\n",
    "                                'SizeInGB': 10\n",
    "                            },\n",
    "                            'VolumesPerInstance': 1\n",
    "                        },\n",
    "                    ],\n",
    "                    'EbsOptimized': False\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "        'Ec2KeyName':'EMR Key Pair'\n",
    "    },\n",
    "    VisibleToAllUsers=True,\n",
    "    ServiceRole='EMR_DefaultRole',\n",
    "    JobFlowRole='EMR_EC2_DefaultRole',\n",
    "    AutoScalingRole=\"EMR_AutoScaling_DefaultRole\"\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    '''Create SparkSession object to read/write data with Spark.'''\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Unzip data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data/log-data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')\n",
    "    \n",
    "with zipfile.ZipFile('data/song-data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Processing a SINGLE song_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_path = \"data/...\"\n",
    "user_log = spark.read.json(song_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Processing a SINGLE log_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_path = \"data/...\"\n",
    "user_log = spark.read.json(log_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
